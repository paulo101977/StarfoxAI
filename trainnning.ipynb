{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d34804-70a0-4608-8922-7b4a3cd105bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States for StarFox-Snes: ['ControlA.MidRoute.Corneria']\n",
      "None finded, starting from zero.\n",
      "Model saved in: model-starfox/best_model_1000\n",
      "Time steps: 1000, Average Reward: 0.002, Best Reward: 0.5\n",
      "Model saved in: model-starfox/best_model_2000\n",
      "Time steps: 2000, Average Reward: 0.0013054830287206266, Best Reward: 0.5\n"
     ]
    }
   ],
   "source": [
    "import retro\n",
    "\n",
    "import gymnasium as gym\n",
    "import re\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym.wrappers import GrayScaleObservation, FrameStack\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3.common.atari_wrappers import ClipRewardEnv, WarpFrame\n",
    "\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "\n",
    "def custom_schedule(progress_remaining):\n",
    "    #return 1e-4 + (3e-4 - 1e-4) * (1 - progress_remaining)\n",
    "    return 5e-5 + (2e-4 - 5e-5) * (1 - progress_remaining)\n",
    "\n",
    "# Model Param\n",
    "CHECK_FREQ_NUMB = 1000\n",
    "TOTAL_TIMESTEP_NUMB = 500_000_000\n",
    "LEARNING_RATE = custom_schedule # 0.00025 # 0.0002 # 0.0001 # 0.00025 # 0.0001\n",
    "GAE = 0.98 # 0.9 # 1.0 # 0.95 # 1.0\n",
    "ENT_COEF = 0.004 # 0.001 # 0.03 # 0.01 # 0.03 # 0.1 # 0.03 # 0.02 # 0.01 # 0.005 # 0.01\n",
    "N_STEPS = 2048 # 4096 # 512 # 2048 # 4096 # 2048 # 512\n",
    "GAMMA = 0.99 # 0.9\n",
    "BATCH_SIZE = 512 # 128 # 64\n",
    "CLIP_RANGE = 0.1 # 0.15 # 0.2 # 0.4 # 0.3\n",
    "N_EPOCHS = 10 # 6 # 10 # 15 # 10\n",
    "MAX_EPISODE=0# 15000\n",
    "USE_CURRICULUM=False\n",
    "USE_CLIP_REWARD=False\n",
    "STATE=\"ControlA.MidRoute.Corneria\"\n",
    "TENSORBOARD=\"./tensorboard-starfox\"\n",
    "SAVE_DIR=\"./model-starfox\"\n",
    "NUM_ENV = 16\n",
    "\n",
    "model = None\n",
    "\n",
    "# Test Param\n",
    "EPISODE_NUMBERS = 20\n",
    "SAVE_FREQ=1000\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "GAME = \"StarFox-Snes\"\n",
    "states = retro.data.list_states(GAME)\n",
    "\n",
    "# print(retro.data.list_games())\n",
    "print(f\"States for {GAME}: {states}\")\n",
    "\n",
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "class MainDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    Use Sonic-specific discrete actions\n",
    "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            combos=[\n",
    "                [\"LEFT\"],         # rotate to left\n",
    "                [\"RIGHT\"],        # rotate to right\n",
    "                [\"UP\"],          \n",
    "                [\"DOWN\"],         \n",
    "                [\"B\"],         # shoot  \n",
    "                [\"Y\"],           \n",
    "                [\"L\"],        # tilt to left\n",
    "                [\"R\"],        # tilt to right \n",
    "                [\"LEFT\", \"B\"],    # shoot left\n",
    "                [\"RIGHT\", \"B\"],   # shoot right\n",
    "                [\"UP\", \"B\"],      # shoot up\n",
    "                [\"DOWN\", \"B\"],   # shoot down\n",
    "                [\"L\", \"LEFT\"],    # tilt to left and go to left\n",
    "                [\"R\", \"RIGHT\"],   # tilt to right and go to right\n",
    "                [\"L\", \"RIGHT\"],   # tilt to left and go to right\n",
    "                [\"R\", \"LEFT\"],    # tilt to right and go to left\n",
    "                [\"L\", \"LEFT\", \"B\"],  # tilt to left and go to left and fire\n",
    "                [\"R\", \"RIGHT\", \"B\"], # tilt to right and go to right and fire\n",
    "            ]\n",
    "        )\n",
    "\n",
    "class IgnorePauseActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, act):\n",
    "        act[3] = 0\n",
    "        return act\n",
    "\n",
    "\n",
    "class ResetStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "        self.steps = 0\n",
    "        self.lose_lives = False\n",
    "        self.current_health = 40\n",
    "        self.current_lives = 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        #self.x_last = self.env.unwrapped.data['x']\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "        self.current_health = 40\n",
    "\n",
    "        self.current_lives = 3\n",
    "\n",
    "        self.lose_lives = False\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # reward = 0\n",
    "        if reward > 0:\n",
    "            reward = 0.5 # new score reward\n",
    "\n",
    "        # Existential to encourage staying alive longer\n",
    "        # reward += 0.05\n",
    "\n",
    "        # 'health': 36, 'kills': 15, 'lives': 2,\n",
    "        health = info['health']\n",
    "        lives = info['lives']\n",
    "\n",
    "        # Reach Max Trainning Step\n",
    "        if MAX_EPISODE > 0 and self.steps > MAX_EPISODE:\n",
    "            done = True\n",
    "            reward -= 2\n",
    "\n",
    "        # lost lives\n",
    "        if self.current_lives > info['lives']:\n",
    "            reward -= 2\n",
    "            self.current_lives =  info['lives']\n",
    "            done = True\n",
    "\n",
    "        # win lives\n",
    "        if info['lives'] > self.current_lives:\n",
    "            reward += 1\n",
    "            self.current_lives =  info['lives']\n",
    "\n",
    "        # lose\n",
    "        if info['lives'] < 1:\n",
    "            reward -= 2\n",
    "            done = True\n",
    "\n",
    "        # lost shield\n",
    "        if info['health'] < self.current_health:\n",
    "            reward -= 0.5\n",
    "            self.current_health = info['health']\n",
    "\n",
    "        if info['health'] > self.current_health:\n",
    "            reward += 0.5\n",
    "            self.current_health = info['health']\n",
    "\n",
    "        # if done and not self.lose_lives:\n",
    "        #     reward += 2\n",
    "\n",
    "        \n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "class RandomStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "    def get_random_state(self):\n",
    "        \"\"\"Select a random state from folder STATE_PATH\"\"\"\n",
    "        STATE_PATH = \"./States\"\n",
    "        states = [f for f in os.listdir(STATE_PATH) if f.endswith(\".state\")]\n",
    "        if not states:\n",
    "            raise FileNotFoundError(\"File not found!\")\n",
    "        c = random.choice(states)\n",
    "\n",
    "        return os.path.abspath(\"./States/\" + c)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        state = self.get_random_state()\n",
    "        #print(f\"Loading state: {state}\")\n",
    "        self.env.load_state(state)\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        if done or trunc:\n",
    "            self.reset()\n",
    "\n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "\n",
    "class CurriculumWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, required_wins=20): #required_avg_reward=1.0):\n",
    "        super().__init__(env)\n",
    "        self.required_wins = required_wins\n",
    "        #self.required_avg_reward = required_avg_reward\n",
    "        self.current_phase = 1\n",
    "        self.total_wins = 0 \n",
    "        self.rewards_list = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        self.rewards_list.append(reward)\n",
    "\n",
    "        could_to_next_stage = info[\"matches_won\"] / self.current_phase >= 2\n",
    "\n",
    "        if info[\"matches_won\"] % 2 == 0 and info[\"matches_won\"] > 0 and could_to_next_stage:\n",
    "            self.total_wins += 1\n",
    "\n",
    "\n",
    "        # avg_reward = np.mean(self.rewards_list[-self.required_wins:]) if len(self.rewards_list) >= self.required_wins else np.mean(self.rewards_list)\n",
    "        avg_reward = np.mean(self.rewards_list)\n",
    "\n",
    "        if could_to_next_stage and \\\n",
    "            ((info[\"matches_won\"] % 2 == 0  and info[\"matches_won\"] > 0) \\\n",
    "                 or (info[\"enemy_matches_won\"] % 2 == 0 and info[\"enemy_matches_won\"] > 0)) :\n",
    "            print(info)\n",
    "            print(f\"🔥 stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            done = True\n",
    "        \n",
    "        if self.total_wins >= self.required_wins: #and avg_reward >= self.required_avg_reward:\n",
    "            self.current_phase += 1\n",
    "            print(f\"🔥 Going to next stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            self.total_wins = 0\n",
    "            self.rewards_list = []\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            observation, reward, terminated, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return observation, total_reward, terminated, trunk, info\n",
    "\n",
    "\n",
    "class GameNet(BaseFeaturesExtractor):\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim):\n",
    "        super(GameNet, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_freq=SAVE_FREQ,\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.best_reward = float('-inf')\n",
    "        self.episode_rewards = []\n",
    "        self.current_episode_reward = 0\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        reward = self.locals[\"rewards\"][0]\n",
    "        self.current_episode_reward = reward\n",
    "\n",
    "        done = self.locals[\"dones\"][0]\n",
    "\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "\n",
    "        if done:\n",
    "            self.current_episode_reward = 0\n",
    "            self.episode_rewards = []\n",
    "            self.best_reward = float('-inf')\n",
    "        \n",
    "        if self.n_calls % self.check_freq == 0 and len(self.episode_rewards) > 0:\n",
    "            latest_model = get_latest_model(self.save_path)\n",
    "            next_save_step = (int(re.search(r\"best_model_(\\d+)\", str(latest_model)).group(1)) + self.check_freq) if latest_model else self.n_calls\n",
    "            model_path = self.save_path / f\"best_model_{next_save_step}\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved in: {model_path}\")\n",
    "\n",
    "            average_reward = sum(self.episode_rewards) / len(self.episode_rewards)\n",
    "            best_reward = max(self.episode_rewards)\n",
    "            sum_rewards = sum(self.episode_rewards)\n",
    "\n",
    "            self.best_reward = max(self.best_reward, best_reward)\n",
    "\n",
    "            self.logger.record(\"average_reward\", average_reward)\n",
    "            self.logger.record(\"best_reward\", self.best_reward)\n",
    "            self.logger.record(\"sum_rewards\", sum_rewards)\n",
    "\n",
    "            if USE_CURRICULUM:\n",
    "                self.logger.record(\"current_phase\", self.training_env.get_attr(\"current_phase\")[0])\n",
    "\n",
    "            print(f\"Time steps: {self.n_calls}, Average Reward: {average_reward}, Best Reward: {self.best_reward}\")\n",
    "\n",
    "\n",
    "        return True\n",
    "          \n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=GameNet,\n",
    "    features_extractor_kwargs=dict(features_dim=1024), # features_extractor_kwargs=dict(features_dim=512),\n",
    "    net_arch=dict(\n",
    "        pi=[1024, 512, 256],  # Actor\n",
    "        vf=[1024, 1024, 512]  # Critic\n",
    "    ) #\n",
    ")\n",
    "\n",
    "def get_latest_model(path):\n",
    "    models = list(path.glob(\"best_model_*\"))\n",
    "    if not models:\n",
    "        return None\n",
    "    model_numbers = [int(re.search(r\"best_model_(\\d+)\", str(m)).group(1)) for m in models]\n",
    "    latest_model = max(model_numbers)\n",
    "    return path / f\"best_model_{latest_model}\"\n",
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = retro.make(\n",
    "            game=GAME, \n",
    "            #use_restricted_actions=retro.Actions.DISCRETE, \n",
    "            # render_mode=\"human\",\n",
    "            render_mode=None,\n",
    "            state=STATE\n",
    "        )\n",
    "\n",
    "        # env = DonkeyKongCustomActions(env)\n",
    "\n",
    "        env = MainDiscretizer(env)\n",
    "\n",
    "        # env = IgnorePauseActionWrapper(env)\n",
    "\n",
    "        # env = RandomStateWrapper(env)\n",
    "\n",
    "        env = ResetStateWrapper(env)\n",
    "        \n",
    "        env = SkipFrame(env, skip=4)\n",
    "        env = WarpFrame(env)\n",
    "\n",
    "        if USE_CURRICULUM:\n",
    "            env = CurriculumWrapper(env, required_wins=50) #, required_avg_reward=0.6)\n",
    "\n",
    "        if USE_CLIP_REWARD:\n",
    "            env = ClipRewardEnv(env)\n",
    "\n",
    "        #env = TimeLimit(env, max_episode_steps=MAX_EPISODE)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "env = SubprocVecEnv([make_env() for _ in range(NUM_ENV)])\n",
    "# env = DummyVecEnv([make_env()])\n",
    "# env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "checkpoint_callback=TrainAndLoggingCallback(check_freq=CHECK_FREQ_NUMB, save_path=save_dir)\n",
    "\n",
    "\n",
    "latest_model_path = get_latest_model(save_dir)\n",
    "\n",
    "if latest_model_path:\n",
    "    print(f\"Loading existent model: {latest_model_path}\")\n",
    "    model = PPO.load(\n",
    "        str(latest_model_path), \n",
    "        env=env, \n",
    "        verbose=0, \n",
    "        tensorboard_log=TENSORBOARD, \n",
    "        learning_rate=LEARNING_RATE, \n",
    "        n_steps=N_STEPS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        n_epochs=N_EPOCHS, \n",
    "        gamma=GAMMA, \n",
    "        gae_lambda=GAE, \n",
    "        clip_range=CLIP_RANGE,\n",
    "        ent_coef=ENT_COEF,\n",
    "        policy_kwargs=policy_kwargs, \n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"None finded, starting from zero.\")\n",
    "    model = PPO('CnnPolicy', \n",
    "                env, \n",
    "                verbose=0, \n",
    "                policy_kwargs=policy_kwargs, \n",
    "                tensorboard_log=TENSORBOARD, \n",
    "                learning_rate=LEARNING_RATE, \n",
    "                n_steps=N_STEPS, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                n_epochs=N_EPOCHS, \n",
    "                gamma=GAMMA, \n",
    "                gae_lambda=GAE, \n",
    "                clip_range=CLIP_RANGE,\n",
    "                ent_coef=ENT_COEF\n",
    "            )\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB, reset_num_timesteps=False, callback=checkpoint_callback)\n",
    "model.save(\"starfox_final\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0005f91-782c-4ace-9c14-76604df99d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
